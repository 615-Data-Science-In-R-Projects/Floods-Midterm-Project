---
title: "Floods"
author: "Ajay Krishnakumar"
date: 2023 Nov 6
format: pdf
engine: knitr
---

```{r}
#| echo: false
#| message: false
#Initializing relevant libraries

library(tidyverse)
library(ggplot2)
```

# Introduction

#Data Cleaning

```{r}
#| echo: false
#| message: false

#Reading in the data from NOAA
storms_2021<- read.csv("StormEvents_details_2021.csv")
storms_2020<- read.csv("StormEvents_details_2020.csv")

vec_20<- rep("2020",nrow(storms_2020))
vec_21<- rep("2021",nrow(storms_2021))

storms_2021<- storms_2021 |> mutate("YEAR"=vec_21)
storms_2020<- storms_2020 |> mutate("YEAR"=vec_20)

storms<- rbind(storms_2021,storms_2020)

#Filtering so that we only look at storms:
floods<- storms |> filter(str_detect(EVENT_TYPE,"Flood")==TRUE)

```

So immediately, a few things stand out: the way the dates are coded and the fact that they're coded twice and differently. What I'll shoot for here is to have dates as month, day and time and also as a single datetime

```{r}
#| echo: false

#We'll start by cleaning begin date and end date to date-time format so we can join with the fema data

floods1<- floods |> mutate(incidentBeginDate= 
            dmy(str_sub(BEGIN_DATE_TIME,1,
          str_locate(BEGIN_DATE_TIME," ")[2])),.before =BEGIN_DATE_TIME)

floods1<- floods1 |> mutate(incidentEndDate= 
            dmy(str_sub(END_DATE_TIME,1,
          str_locate(END_DATE_TIME," ")[2])),.before =END_DATE_TIME)

#We can also add a time column. We'll adjust this for timezones later

floods1<- floods1 |> mutate(incidentBeginTime = 
          hms(str_sub(BEGIN_DATE_TIME,
          str_locate(BEGIN_DATE_TIME," ")[2],)), .before=BEGIN_DATE_TIME)

floods1<- floods1 |> mutate(incidentEndTime = 
          hms(str_sub(END_DATE_TIME,
          str_locate(END_DATE_TIME," ")[2],)), .before=END_DATE_TIME)
```

After dates, the damages columns need fixing.

```{r}
#| echo: false

#Property damage

#First lets check if there are any entries in property damage that don't end in
#either K or M
floods1$DAMAGE_PROPERTY<- str_trim(floods$DAMAGE_PROPERTY, side="both")
dim(floods |> filter(str_detect(DAMAGE_PROPERTY,"K")==TRUE |
                       str_detect(DAMAGE_PROPERTY,"M")==TRUE))[1]

#This isn't the same as the number of rows in floods. A little messing around (looking at distinct values for property damage) tells us that this is because some values are NA. 

#So now we know that every entry in this column ends in k or m or is NA, we can 
#retrieve the number and conditionally multiply by 1000 or a million

floods1$DAMAGE_PROPERTY<- ifelse(str_detect(floods$DAMAGE_PROPERTY,"K")==TRUE,
                    as.numeric(str_sub(floods$DAMAGE_PROPERTY,0,-2))*1000,
                    ifelse(str_detect(floods$DAMAGE_PROPERTY,"M")==TRUE,
                    as.numeric(str_sub(floods$DAMAGE_PROPERTY,0,-2))*1000000,
                    NA))

#We can do the same for crop_damage

floods1$DAMAGE_CROPS<- ifelse(str_detect(floods$DAMAGE_CROPS,"K")==TRUE,
                    as.numeric(str_sub(floods$DAMAGE_CROPS,0,-2))*1000,
                    ifelse(str_detect(floods$DAMAGE_CROPS,"M")==TRUE,
                    as.numeric(str_sub(floods$DAMAGE_CROPS,0,-2))*1000000,
                    NA))

```

Let's join the floods data with the combined fema data:

```{r}
#| echo: false
#| warning: false
fema<- suppressWarnings(read_csv("DisasterDeclarationsSummaries.csv"))
fema_floods<- fema |> filter(str_detect(incidentType, "Flood")==TRUE) |> 
                  filter(str_detect(incidentBeginDate,"2021")==TRUE|
                           str_detect(incidentBeginDate,"2020")==TRUE)

fema_assistance<- suppressWarnings(read.csv("FemaWebDisasterSummaries.csv", header = TRUE))
fema_combined<- suppressWarnings(left_join(x=fema_floods,y=fema_assistance, by="disasterNumber"))

fema_combined<-fema_combined |> select(-c(lastRefresh.x,lastRefresh.y,hash.x,hash.y,id.x, id.y))

#We'll start by trying to join by begin and end dates. 

floods_fema_combined<- suppressWarnings(
  left_join(x=floods1, y=fema_combined, by="incidentBeginDate"))

#removing redundant columns

floods_fema_combined<-floods_fema_combined |> 
  select(-c(BEGIN_YEARMONTH:END_TIME,YEAR,MONTH_NAME,MAGNITUDE,
                                  MAGNITUDE_TYPE,CATEGORY:TOR_OTHER_CZ_NAME))

#Filtering out columns for which we don't have fema data

floods_fema_combined_noNA<- floods_fema_combined |>
  filter(is.na(declarationType)==FALSE)
```

We can also join this data with poverty data from the census. This data has to be cleaned first:

```{r}
#| echo: false
#| warning: false
#We start by reading in the data
census1_2020<- read.csv("Census Download_2023-10-23T135225/ACSST5Y2020.S1701-Data.csv")
census1_2021<- read.csv("Census Download_2023-10-23T135225/ACSST5Y2021.S1701-Data.csv")
census1_metadata<- read.csv("Census Download_2023-10-23T135225/ACSST5Y2020.S1701-Column-Metadata.csv")
census_2021_metadata<- read.csv("Census Download_2023-10-23T135225/ACSST5Y2021.S1701-Column-Metadata.csv")

#We will keep the census estimates and the margins of error for each estimate
census1_estimates_2020<- census1_2020 |>
        select(colnames(census1_2020)[which(str_sub(colnames(census1_2020),-1,-1)=="E" |
                                         str_sub(colnames(census1_2020),-1,-1)=="M")])

census1_estimates_2021<- census1_2021 |>
  select(colnames(census1_2021)[which(str_sub(colnames(census1_2021),-1,-1)=="E" |
                                        str_sub(colnames(census1_2021),-1,-1)=="M")])

#Differentiating the years by adding a year column
vec_2020<- rep("2020",3222)
vec_2021<-rep("2021",3222)

census1_estimates_2020<- census1_estimates_2020 |> mutate(year=vec_2020)
census1_estimates_2021<- census1_estimates_2021 |> mutate(year=vec_2021)

#removing variables of b which aren't in a. Having looked at what these are, these aren't questions i'm interested in so I can omit them. 

col_2020<-colnames(census1_estimates_2020)
col_2021<- colnames(census1_estimates_2021)

diff<- col_2021[which((col_2021%in%col_2020)==FALSE)]
census1_estimates_2021<- census1_estimates_2021 |> select(-diff)
census_poverty_data<- rbind(census1_estimates_2020,census1_estimates_2021)

#Cleaning the column names:

colnames(census_poverty_data)<- census_poverty_data[1,]
#Removing the label so we just have column names
census_poverty_data<-census_poverty_data[2:6444,]

col_names<- colnames(census_poverty_data)

col_names<- str_replace_all(col_names,"!!", " ")
col_names<- str_replace_all(col_names,"Total Population for whom poverty status is determined",
                                       "Total Population")

col_names<- str_replace_all(col_names,"Population for whom poverty status is determined",
                            "")

col_names<- ifelse(str_detect(col_names,"AGE")==TRUE,str_replace(col_names,
  "Population for whom poverty status is determined AGE", "AGE"),col_names)
col_names_frame<- data.frame(col_names)

#Replacing age sub-categorizations because they're redundant

col_names<-ifelse(str_detect(col_names,"Under 18 years")==TRUE&
                    str_sub(col_names,-14,-1)!="Under 18 years",
                  str_replace(col_names,"Under 18 years",""),
                  ifelse(str_detect(col_names,"18 to 64 years")==TRUE&
                          str_sub(col_names,-14,-1)!="18 to 64 years" ,
                         str_replace(col_names,"18 to 64 years","" ),
                  col_names))

col_names<- str_replace(col_names,"RACE AND HISPANIC OR LATINO ORIGIN", "RACE")
col_names<- str_replace(col_names,
    "UNRELATED INDIVIDUALS FOR WHOM POVERTY STATUS IS DETERMINED", "UNRELATED INDIVIDUALS")
col_names<- str_replace(col_names,"Population 25 years and over","")
col_names<- str_replace(col_names,"Civilian labor force 16 years and over","")

colnames(census_poverty_data)<- col_names
#Storing the census questions in a data frame so I can consult it in the future when deciding on what analysis is appropriate
metadata_census_qs<- data.frame(col_names)
```

Now, in order to join the census data and the floods combined data, we need to reduce the geographic area name column to just the county. Furthermore, we will rename it to have the same name as in floods.

```{r}
#| echo: false
#| warning: false
census_poverty_data<- census_poverty_data |> 
              mutate(County = 
        toupper(str_trim(str_sub(`Geographic Area Name`,1,
          str_locate(`Geographic Area Name`," ")[,1]),side="both")), 
        .before=`Estimate Total Population`)

census_poverty_data<- census_poverty_data |> mutate(
  State=toupper(str_trim(str_sub(`Geographic Area Name`,
str_locate(`Geographic Area Name`,",")[,1]+1,),side="both")),
        .before=`Estimate Total Population`) |> select(-`Geographic Area Name`)

census_poverty_data[,3:369]<- lapply(census_poverty_data[,3:369],as.numeric)

floods_fema_combined_noNA<- floods_fema_combined_noNA |> 
                      rename(County = CZ_NAME, State = STATE)


floods_fema_census<- left_join(floods_fema_combined_noNA,census_poverty_data,
                               by=c("County","State"))

```

Now we have three data frames - just the floods data from NOAA, the floods and fema data and the floods, fema and census data combined. That means its time for some EDA.

# EDA

So we can start by just looking at the number of flood events in each state over the two years in question. This might point to something interesting or at the very least give some indication of where to focus analysis if we want to have that focus geographically. 
```{r}
#| echo: false
f<- floods_fema_census |> group_by(State) |> count(State) |> arrange(desc(n)) |> rename(NumberOfFloods = n)
print(f[1:12,])
```
We have coastal states like New York and West Virginia represented but it would appear that the large majority of floods happen in non-coastal states. 
What's interesting here is that South Dakota has a large number of flood events. So do Kentucy, Arkansas, Tennessee and Missouri but those latter states are geographically close. South Dakota is a geographic outlier here. My focus, in this analysis, is to look at whether floods disproportionately affect poor areas and what relationship poverty has, if any, with fema claims. It might be worth paying special attention to South Dakota later. 

Let's start by visualizing how poverty varies across the states in question. We can limit ourselves to the top twelve most flooded states as indicated above. 

```{r}
#| echo: false
state_list<- c("KENTUCKY","SOUTH DAKOTA", "TENNESSEE", "ARKANSAS", "MISSOURI", 
               "ILLINOIS", "IOWA","NEW YORK", "INDIANA","WEST VIRGINIA",
               "FLORIDA", "TEXAS")
               


state_poverty_level<-census_poverty_data |> 
  filter(State %in% state_list) |> group_by(State) |> summarise(`Below poverty level` =sum(`Estimate Below poverty level `)/sum(`Estimate Total Population`)*100)

ggplot(state_poverty_level, aes(State, `Below poverty level`))+
  geom_bar(stat="identity", fill="#8FA8F9")+coord_cartesian(ylim=c(10,18))+
  theme_classic()+
  labs(title="Percent of population below poverty level by State", 
       y= "Percent below poverty level", x= "State")+
  geom_hline(yintercept=11.5, color='red')+
  annotate("text", x="KENTUCKY",
           y=11.8, label="US Percent below poverty level = 11.5%")+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
  
  
```
We can see that most of these states, barring Iowa, are above the US Percent of population below poverty level.(Congressional Research Service, Poverty in the United States 2021 and 2020) Let's see how that compares  for only those counties with flood events in 2020 or 2021. 

```{r}
#| echo: false
state_list<-  c("KENTUCKY","SOUTH DAKOTA", "TENNESSEE", "ARKANSAS", "MISSOURI", 
               "ILLINOIS", "IOWA","NEW YORK", "INDIANA","WEST VIRGINIA",
               "FLORIDA", "TEXAS")

poverty_noNA<- floods_fema_census |> filter(
  is.na(`Estimate Below poverty level `)==FALSE& 
    is.na(`Estimate Total Population`)==FALSE)

poverty_level<-poverty_noNA |> 
  filter(State %in% state_list) |> group_by(State) |> summarise(`Below poverty level` =sum(`Estimate Below poverty level `)/sum(`Estimate Total Population`)*100) |> mutate(
    poverty_diff= `Below poverty level`- state_poverty_level$`Below poverty level`
  )

ggplot(poverty_level, aes(State, poverty_diff))+
  geom_bar(stat="identity", fill="#8FA8F9")+#coord_cartesian(ylim=c(10,18))+
  theme_classic()+
  labs(title="Poverty levels for flooded areas relative to the Whole State", 
       y= "Difference in Percent below poverty level", x= "State")+
  geom_hline(yintercept=0, color='red')+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```
We can see that there are a few states here where the flooded areas appear poorer than the state and others where the flooded areas aren't as poor as the rest of the state. This doesn't necessarily mean that in these states the floods happen in areas with wealthier people. We can use the census data to look at flooding in areas where the income is high above the poverty level (Which is by no means equivalent to saying areas with high levels of income.)We can look at areas between 300% and  500% of the US poverty level as being comfortably above the poverty level but we'll come to that later.

Are there any trends in deaths, property damage and crop damage between these two groups of states? 

```{r}
#poor is a misnomer here and refers to the flooded areas in each state, not the poverty of the state overall
state_list<-  c("KENTUCKY","SOUTH DAKOTA", "TENNESSEE", "ARKANSAS", "MISSOURI", 
               "ILLINOIS", "IOWA","NEW YORK", "INDIANA","WEST VIRGINIA",
               "FLORIDA", "TEXAS")

poor_list<-c("ARKANSAS", "FLORIDA", "ILLINOIS", "KENTUCKY", "MISSOURI", 
             "NEW YORK", "WEST VIRGINIA")
not_poor_state_list<- c("INDIANA", "IOWA", "SOUTH DAKOTA", "TENNESSEE", "TEXAS")

comparisons_state_groups<- floods_fema_census |> filter(State %in% state_list) |> 
  group_by(State) |> summarize(
    deaths=sum(DEATHS_DIRECT+DEATHS_INDIRECT),
    prop_damage = sum(DAMAGE_PROPERTY),
    crop_damage=sum(DAMAGE_CROPS, .keepall=TRUE)
  ) |> mutate(poorer_areas_flooded = ifelse(State %in% poor_list),1,0)

#It would appear that the data for property and crop damage for Florida is NA. These values are also zero for some other states. 

ggplot(comparisons_state_groups, aes(State,log(prop_damage)))+
  geom_point()
```

