---
title: "Floods"
author: "Ajay Krishnakumar"
date: 2023 Nov 6
format: pdf
engine: knitr
---

```{r}
#| echo: false
#| message: false
#Initializing relevant libraries

library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
```

# Introduction

#Data Cleaning

```{r}
#| echo: false
#| message: false

#Reading in the data from NOAA
storms_2021<- read.csv("StormEvents_details_2021.csv")
storms_2020<- read.csv("StormEvents_details_2020.csv")

vec_20<- rep("2020",nrow(storms_2020))
vec_21<- rep("2021",nrow(storms_2021))

storms_2021<- storms_2021 |> mutate("YEAR"=vec_21)
storms_2020<- storms_2020 |> mutate("YEAR"=vec_20)

storms<- rbind(storms_2021,storms_2020)

#Filtering so that we only look at storms:
floods<- storms |> filter(str_detect(EVENT_TYPE,"Flood")==TRUE)

```

So immediately, a few things stand out: the way the dates are coded and the fact that they're coded twice and differently. What I'll shoot for here is to have dates as month, day and time and also as a single datetime

```{r}
#| echo: false

#We'll start by cleaning begin date and end date to date-time format so we can join with the fema data

floods1<- floods |> mutate(incidentBeginDate= 
            dmy(str_sub(BEGIN_DATE_TIME,1,
          str_locate(BEGIN_DATE_TIME," ")[2])),.before =BEGIN_DATE_TIME)

floods1<- floods1 |> mutate(incidentEndDate= 
            dmy(str_sub(END_DATE_TIME,1,
          str_locate(END_DATE_TIME," ")[2])),.before =END_DATE_TIME)

#We can also add a time column. We'll adjust this for timezones later

floods1<- floods1 |> mutate(incidentBeginTime = 
          hms(str_sub(BEGIN_DATE_TIME,
          str_locate(BEGIN_DATE_TIME," ")[2],)), .before=BEGIN_DATE_TIME)

floods1<- floods1 |> mutate(incidentEndTime = 
          hms(str_sub(END_DATE_TIME,
          str_locate(END_DATE_TIME," ")[2],)), .before=END_DATE_TIME)
```

After dates, the damages columns need fixing.

```{r}
#| echo: false
#| message: false
#| warning: false

#Property damage

#First lets check if there are any entries in property damage that don't end in
#either K or M
floods1$DAMAGE_PROPERTY<- str_trim(floods$DAMAGE_PROPERTY, side="both")
dim(floods |> filter(str_detect(DAMAGE_PROPERTY,"K")==TRUE |
                       str_detect(DAMAGE_PROPERTY,"M")==TRUE))[1]

#This isn't the same as the number of rows in floods. A little messing around (looking at distinct values for property damage) tells us that this is because some values are NA. 

#So now we know that every entry in this column ends in k or m or is NA, we can 
#retrieve the number and conditionally multiply by 1000 or a million

floods1$DAMAGE_PROPERTY<- ifelse(str_detect(floods$DAMAGE_PROPERTY,"K")==TRUE,
                    as.numeric(str_sub(floods$DAMAGE_PROPERTY,0,-2))*1000,
                    ifelse(str_detect(floods$DAMAGE_PROPERTY,"M")==TRUE,
                    as.numeric(str_sub(floods$DAMAGE_PROPERTY,0,-2))*1000000,
                    NA))

#We can do the same for crop_damage

floods1$DAMAGE_CROPS<- ifelse(str_detect(floods$DAMAGE_CROPS,"K")==TRUE,
                    as.numeric(str_sub(floods$DAMAGE_CROPS,0,-2))*1000,
                    ifelse(str_detect(floods$DAMAGE_CROPS,"M")==TRUE,
                    as.numeric(str_sub(floods$DAMAGE_CROPS,0,-2))*1000000,
                    NA))

```

Let's join the floods data with the combined fema data:

```{r}
#| echo: false
#| warning: false
#| message: false
fema<- suppressWarnings(read_csv("DisasterDeclarationsSummaries.csv"))
fema_floods<- fema |> filter(str_detect(incidentType, "Flood")==TRUE) |> 
                  filter(str_detect(incidentBeginDate,"2021")==TRUE|
                           str_detect(incidentBeginDate,"2020")==TRUE)

fema_assistance<- suppressWarnings(read.csv("FemaWebDisasterSummaries.csv", header = TRUE))
fema_combined<- suppressWarnings(left_join(x=fema_floods,y=fema_assistance, by="disasterNumber"))

fema_combined<-fema_combined |> select(-c(lastRefresh.x,lastRefresh.y,hash.x,hash.y,id.x, id.y))

fema_combined$designatedArea<-
   str_trim(toupper(ifelse(str_detect(fema_combined$designatedArea,"County")|str_detect(fema_combined$designatedArea,"Parish"),
          str_sub(fema_combined$designatedArea,1,str_locate(fema_combined$designatedArea," ")),
          fema_combined$designatedArea)),side="both")

fema_combined<-fema_combined |> rename(County=designatedArea)
#We'll start by trying to join by begin and end dates. 

floods1<- floods1 |> rename(County = CZ_NAME)
floods_fema_combined<- suppressWarnings(
  left_join(x=floods1, y=fema_combined, by="County"))

#removing redundant columns

floods_fema_combined<-floods_fema_combined |> 
  select(-c(BEGIN_YEARMONTH:END_TIME,YEAR,MONTH_NAME,MAGNITUDE,
                                  MAGNITUDE_TYPE,CATEGORY:TOR_OTHER_CZ_NAME))

#Filtering out columns for which we don't have fema data

floods_fema_combined_noNA<- floods_fema_combined |>
  filter(is.na(declarationType)==FALSE)
```

We can also join this data with poverty data from the census. This data has to be cleaned first:

```{r}
#| echo: false
#| warning: false
#We start by reading in the data
census1_2020<- read.csv("Census Download_2023-10-23T135225/ACSST5Y2020.S1701-Data.csv")
census1_2021<- read.csv("Census Download_2023-10-23T135225/ACSST5Y2021.S1701-Data.csv")
census1_metadata<- read.csv("Census Download_2023-10-23T135225/ACSST5Y2020.S1701-Column-Metadata.csv")
census_2021_metadata<- read.csv("Census Download_2023-10-23T135225/ACSST5Y2021.S1701-Column-Metadata.csv")

#We will keep the census estimates and the margins of error for each estimate
census1_estimates_2020<- census1_2020 |>
        select(colnames(census1_2020)[which(str_sub(colnames(census1_2020),-1,-1)=="E" |
                                         str_sub(colnames(census1_2020),-1,-1)=="M")])

census1_estimates_2021<- census1_2021 |>
  select(colnames(census1_2021)[which(str_sub(colnames(census1_2021),-1,-1)=="E" |
                                        str_sub(colnames(census1_2021),-1,-1)=="M")])

#Differentiating the years by adding a year column
vec_2020<- rep("2020",3222)
vec_2021<-rep("2021",3222)

census1_estimates_2020<- census1_estimates_2020 |> mutate(year=vec_2020)
census1_estimates_2021<- census1_estimates_2021 |> mutate(year=vec_2021)

#removing variables of b which aren't in a. Having looked at what these are, these aren't questions i'm interested in so I can omit them. 

col_2020<-colnames(census1_estimates_2020)
col_2021<- colnames(census1_estimates_2021)

diff<- col_2021[which((col_2021%in%col_2020)==FALSE)]
census1_estimates_2021<- census1_estimates_2021 |> select(-diff)
census_poverty_data<- rbind(census1_estimates_2020,census1_estimates_2021)

#Cleaning the column names:

colnames(census_poverty_data)<- census_poverty_data[1,]
#Removing the label so we just have column names
census_poverty_data<-census_poverty_data[2:6444,]

col_names<- colnames(census_poverty_data)

col_names<- str_replace_all(col_names,"!!", " ")
col_names<- str_replace_all(col_names,"Total Population for whom poverty status is determined",
                                       "Total Population")

col_names<- str_replace_all(col_names,"Population for whom poverty status is determined",
                            "")

col_names<- ifelse(str_detect(col_names,"AGE")==TRUE,str_replace(col_names,
  "Population for whom poverty status is determined AGE", "AGE"),col_names)
col_names_frame<- data.frame(col_names)

#Replacing age sub-categorizations because they're redundant

col_names<-ifelse(str_detect(col_names,"Under 18 years")==TRUE&
                    str_sub(col_names,-14,-1)!="Under 18 years",
                  str_replace(col_names,"Under 18 years",""),
                  ifelse(str_detect(col_names,"18 to 64 years")==TRUE&
                          str_sub(col_names,-14,-1)!="18 to 64 years" ,
                         str_replace(col_names,"18 to 64 years","" ),
                  col_names))

col_names<- str_replace(col_names,"RACE AND HISPANIC OR LATINO ORIGIN", "RACE")
col_names<- str_replace(col_names,
    "UNRELATED INDIVIDUALS FOR WHOM POVERTY STATUS IS DETERMINED", "UNRELATED INDIVIDUALS")
col_names<- str_replace(col_names,"Population 25 years and over","")
col_names<- str_replace(col_names,"Civilian labor force 16 years and over","")

colnames(census_poverty_data)<- col_names
#Storing the census questions in a data frame so I can consult it in the future when deciding on what analysis is appropriate
metadata_census_qs<- data.frame(col_names)
```

Now, in order to join the census data and the floods combined data, we need to reduce the geographic area name column to just the county. Furthermore, we will rename it to have the same name as in floods.

```{r}
#| echo: false
#| warning: false
census_poverty_data<- census_poverty_data |> 
              mutate(County = 
        toupper(str_trim(str_sub(`Geographic Area Name`,1,
          str_locate(`Geographic Area Name`," ")[,1]),side="both")), 
        .before=`Estimate Total Population`)

census_poverty_data<- census_poverty_data |> mutate(
  State=toupper(str_trim(str_sub(`Geographic Area Name`,
str_locate(`Geographic Area Name`,",")[,1]+1,),side="both")),
        .before=`Estimate Total Population`) |> select(-`Geographic Area Name`)

census_poverty_data[,3:369]<- lapply(census_poverty_data[,3:369],as.numeric)

floods_fema_combined_noNA<- floods_fema_combined_noNA |> 
                      rename( State = STATE)


floods_fema_census<- left_join(floods_fema_combined_noNA,census_poverty_data,
                               by=c("County","State"))

```

Now we have three data frames - just the floods data from NOAA, the floods and fema data and the floods, fema and census data combined. That means its time for some EDA.

# EDA

So we can start by just looking at the number of flood events in each state over the two years in question. This might point to something interesting or at the very least give some indication of where to focus analysis geographically. It'll help if we can identify trends in relationship between poverty and floods across states and if there's any obviously similar characteristics across states.

```{r}
#| echo: false
f<- floods_fema_census |> group_by(State) |> count(State) |> arrange(desc(n)) |> rename(`Number Of Floods` = n)
f[1:12,] |> kbl(caption = "Table 1: Floods in 2020 and 2021") %>%
  kable_classic(full_width = F)
```

We have coastal states like New York and West Virginia represented but it would appear that the large majority of floods happened in non-coastal states in 2020 and 2021.

What's interesting here is that South Dakota has a large number of flood events. So do Kentucky, Arkansas, Tennessee and Missouri but those latter states are geographically close. South Dakota is a geographic outlier here. My focus, in this analysis, is to look at whether floods disproportionately affect poor areas and what relationship poverty has, if any, with fema claims. It might be worth paying special attention to South Dakota at some other point.

### EDA - Intial Scouting (Noah looking out from the Ark)

Let's start by visualizing how poverty varies across the states in question. We can limit ourselves to the top twelve most flooded states as indicated in Table 1.

```{r}
#| echo: false
state_list<- c("KENTUCKY","SOUTH DAKOTA", "TENNESSEE", "ARKANSAS", "MISSOURI", 
               "ILLINOIS", "IOWA","NEW YORK", "INDIANA","WEST VIRGINIA",
               "FLORIDA", "TEXAS")
               


state_poverty_level<-census_poverty_data |> 
  filter(State %in% state_list) |> group_by(State) |> summarise(`Below poverty level` =sum(`Estimate Below poverty level `)/sum(`Estimate Total Population`)*100)

ggplot(state_poverty_level, aes(State, `Below poverty level`))+
  geom_bar(stat="identity", fill="#8FA8F9")+coord_cartesian(ylim=c(10,18))+
  theme_classic()+
  labs(title="Percent of population below poverty level by State", 
       y= "Percent below poverty level", x= "State")+
  geom_hline(yintercept=11.5, color='red')+
  annotate("text", x="KENTUCKY",
           y=11.8, label="Nation-wide Percent below poverty level = 11.5%")+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
  
  
```

We can see that most of these states, barring Iowa, are above the US Percent of population below poverty level.(Congressional Research Service, Poverty in the United States 2021 and 2020) Let's see how that compares for only those counties with flood events in 2020 or 2021.

```{r}
#| echo: false
state_list<-  c("KENTUCKY","SOUTH DAKOTA", "TENNESSEE", "ARKANSAS", "MISSOURI", 
               "ILLINOIS", "IOWA","NEW YORK", "INDIANA","WEST VIRGINIA",
               "FLORIDA", "TEXAS")


poverty_noNA<- floods_fema_census |> filter(
  is.na(`Estimate Below poverty level `)==FALSE& 
    is.na(`Estimate Total Population`)==FALSE)

poverty_level<-poverty_noNA |> 
  filter(State %in% state_list) |> group_by(State) |> summarise(`Below poverty level` =sum(`Estimate Below poverty level `)/sum(`Estimate Total Population`)*100) |> mutate(
    poverty_diff= `Below poverty level`- state_poverty_level$`Below poverty level`
  )

ggplot(poverty_level, aes(State, poverty_diff))+
  geom_bar(stat="identity", fill="#8FA8F9")+#coord_cartesian(ylim=c(10,18))+
  theme_classic()+
  labs(title="Poverty levels for areas with recorded disasters\n relative to the Whole State", 
       y= "Difference in Percent below poverty level", x= "State")+
  geom_hline(yintercept=0, color='red')+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```

There are some states where the areas affected by floods are poorer on average than the rest of the state. Equivalently there are some where the flooded areas are less poor. But this graph is not in itself all that informative. Instead let's look at percent of population between 300-500% of the poverty line versus those below the poverty line to see if there is a disproportionate relationship between disaster declaration and wealth. There are also many differences between states, their livelihoods, the distribution of the population. It might make sense to narrow our EDA to some states that share not just apparently similar qualities from an economic standpoint but also in the way floods have affected them in 2020-2021.

So: Are there any trends in deaths, property damage and crop damage between the two groups of states we've created above? Can this help us\
We'll take Missouri, Tennessee and Kentucky in the former group because the margins for them are within the limit where errors in measurement would swing them to be in the other group.

```{r}
#| echo: false
#| warning: false
#| message: false
#poor is a misnomer here and refers to the flooded areas in each state, not the poverty of the state overall
state_list<-  c("KENTUCKY","SOUTH DAKOTA", "TENNESSEE", "ARKANSAS", "MISSOURI", 
               "ILLINOIS", "IOWA","NEW YORK", "INDIANA","WEST VIRGINIA",
               "FLORIDA", "TEXAS")

poor_list<-c( "ARKANSAS","ILLINOIS", "INDIANA", "IOWA", "TEXAS", "TENNESSEE",
              "MISSOURI", "KENTUCKY","WEST VIRGINIA")


comparisons_state_groups<- floods_fema_census |> filter(State %in% state_list) |> 
  group_by(State)  |> 
  select(c("State", "County","INJURIES_INDIRECT", "INJURIES_DIRECT", 
    "DEATHS_INDIRECT","DEATHS_DIRECT", "DAMAGE_PROPERTY", "DAMAGE_CROPS" ))|> 
  mutate(poorer_areas_flooded = 
                              ifelse(State %in% poor_list,"Flooded Areas not Richer than State Average","Flooded Areas Richer than State Average"),
         Deaths = DEATHS_INDIRECT+DEATHS_DIRECT) |> group_by(County)

#It would appear that the data for property and crop damage for Florida is NA. These values are also zero for some other states. 

ggplot(comparisons_state_groups, aes(State,log(DAMAGE_PROPERTY)))+
  geom_point(size=1, alpha=0.05)+
  stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), 
        geom="errorbar", color="red", width=0.3)+
  stat_summary(
    geom = "point",
    fun.y = "mean",
    size = 3,
    color="black",
    shape = 23,
    fill = "orange"
  )+
  theme_classic()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))+
  labs(title="Is Property Damage higher in states where floods affect \nareas richer than the state average",
       y= "Property Damage (log)",
         caption = "Orange diamonds indicate the mean log property damage, error bars are in red")+
  facet_wrap(~poorer_areas_flooded)

ggplot(comparisons_state_groups, aes(State,log(DAMAGE_CROPS)))+
  geom_point(size=1, alpha=0.5)+
  stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), 
        geom="errorbar", color="red", width=0.3)+
  stat_summary(
    geom = "point",
    fun.y = "mean",
    size = 3,
    color="black",
    shape = 23,
    fill = "orange"
  )+
  theme_classic()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))+
  labs(title="Is Crop Damage higher in states where floods affect \nareas richer than the state average",
       y= "Crop Damage (log)",
         caption = "Orange diamonds indicate the mean log Crop damage, error bars are in red")+
  facet_wrap(~poorer_areas_flooded)
  

```

The crop damage graph is almost immediately indicative of which states are more rural/rely ostensibly more on farming. What's interesting is the difference in crop damage across states. Is the crop damage higher because the flooding was more sever/more prolonged or was it because the primary crop grown there is more susceptible to flood damage?

The deaths from floods in 2020-2021 were zero in most states which is great news. However it appears Tennessee really suffered, with close to 20 deaths from one flood alone. So this wasn't really worth visualizing.

By far the more interesting graph is the one detailing property damage.It makes sense that Florida, might have had more property damage, being more densely populated than some of these other states and more urbanized. West Virginia had a lot of floods but had lower property damage levels than Florida's two floods and its mean property damage is the lowest of all these states. Tennessee, West Virginia and Kentucky are the most interesting of this bunch. All three have had a number of floods which span a large range of property damage values. Could this be perhaps because the floods have occurred across both rural and urban areas? Or is it perhaps because valuable infrastructure was damaged in the floods - roads, farming equipment and their like? It might make sense to narrow focus again to Tennessee, West Virginia and Kentucky, and look at differences in poverty levels across their counties to see what we can find.

### EDA - Diluvial Details

So, what questions has our initial exploration left us with:

-   Do poorer areas get affected worse by floods?
-   Is property damage damage largely a function of damage to individual homes or to infrastructure?

```{r}

poverty_analysis<-floods_fema_census |> filter(State %in% c("TENNESSEE", "WEST VIRGINIA", "KENTUCKY"))

poverty_analysis<- poverty_analysis |> mutate(percent_poor=
    (`Estimate Below poverty level `/`Estimate Total Population`)*100)|> 
 select(c(County, State, BEGIN_DATE_TIME:FLOOD_CAUSE,femaDeclarationString:totalObligatedAmountHmgp,percent_poor))
hist(x=poverty_analysis$percent_poor, col="#8FA8F9", main= "Histogram of poverty percentage vs Flood incidence", xlab = "Percent of Population below poverty level")

```
There doesn't seem to be a real trend showing that areas with a higher percentage of inhabitants below the poverty line are hit more frequently by floods. This doesn't mean they aren't hit harder. How would we go about seeing if that is the case? We can look at the fema information. How many requests for funding came in?
How are funds distributed across various poverty levels to a) infrastructure b) individuals assistance c) Housing Assistance. 

```{r}
#| echo: false
#| warning: false
#| message: false
poverty_analysis<-floods_fema_census |> filter(State %in% c("TENNESSEE", "WEST VIRGINIA", "KENTUCKY"))

poverty_analysis_severity<- poverty_analysis|> group_by(County) |> summarise(percent_poor =sum(`Estimate Below poverty level `)/sum(`Estimate Total Population`)*100,
              State = State,
               individual_housing_assistance_declared=sum(ihProgramDeclared),
               public_assistance_declared = sum(paProgramDeclared),
               hazard_mitigation_declared=sum(hmProgramDeclared) )

poverty_analysis_severity<- poverty_analysis_severity |> mutate(poverty = 
                        ifelse(percent_poor<10, "Below 10%", 
                        ifelse(percent_poor<15&percent_poor>=10,"10-15%",
                        ifelse(percent_poor<20&percent_poor>=15,"15-20%",
                        ifelse(percent_poor<25&percent_poor>=20,"20-25%",
                        ifelse(percent_poor<30&percent_poor>=25,"25-30%",
                        ">30%")))))) |> distinct(County, .keep_all = TRUE) 

ggplot(poverty_analysis_severity, aes(x=County, y=individual_housing_assistance_declared, color=State))+
geom_point()+facet_wrap(~poverty)+labs(
  title="Declarations of Individual Housing Program Assistance \n by County and Poverty",
  y="Number of IHA Declarations",
  caption="Each point is a county"
)+theme(axis.text.x=element_blank())+scale_color_manual()

```

